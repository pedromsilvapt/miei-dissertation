
\documentclass[a4paper,UKenglish,cleveref, autoref]{oasics-v2019}

\bibliographystyle{plainurl}
\usepackage{booktabs}
\usepackage{fancyvrb}
\fvset{fontsize=\small,commandchars=\\\{\}}


\def\ourtitle{Semantics of Implicitly Timed Musical Events}
\title{\ourtitle}
\titlerunning{\ourtitle}

\author{Pedro M. Silva}{Dummy University Computing Laboratory, Portugal \and My second affiliation, Country \and \url{http://www.myhomepage.edu} }{johnqpublic@dummyuni.org}{https://orcid.org/0000-0002-1825-0097}{(Optional) author-specific funding acknowledgements}

\author{José João Almeida}%
       {Algoritmi, Departamento de Informática, Universidade do Minho, Braga, Portugal}%
       {jj@di.uminho.pt}%
       {https://orcid.org/0000-0002-0722-2031}
       {}
       
\authorrunning{Pedro M. Silva and J.\,J. Almeida}
\Copyright{Pedro Miguel Silva, José João Almeida}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010179.10010186</concept_id>
<concept_desc>Computing methodologies~Language resources</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Language resources}

\keywords{Umbundu, Angola Languages, Morphological Analysis, Spell Checking}

\funding{This research was partially funded by Portuguese National funds
(PIDDAC), through the FCT – Fundação para a Ciência e Tecnologia and
FCT/MCTES under the scope of the projects UIDB/05549/2020 and
UIDB/00319/2020.  Bernardo Sacanene acknowledges from the Angolan
govenment his PhD grant, through INAGBE (Instituto Nacional de Gestão
de Bolsas de Estudos).}

%\nolinenumbers %uncomment to disable line numbering

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (SLATE 2020)}
\EventShortTitle{SLATE 2020}
\EventAcronym{SLATE}
\EventYear{2020}
\EventDate{December 24--27, 2020}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
  In this paper, we'll discuss a simple approach to integrating musical events, such as notes or chords, into a programming language. First we'll analyze the problem and its particular requirements. Then we will discuss the solution we developed to meet those requirements. Finally we'll analyze the result and discuss possible alternative routes we could've taken.
\end{abstract}



\section{Introduction}
Programming languages are sometimes described as data plus code. However, it is a known fact there is some overlap between the two. While a big chunk of data most programs consume is ingested through some sort of \textit{IO} operation (like reading from a keyboard, a file or a socket), and another chunk is generated dynamically by algorithms at runtime, there is a third chunk: smaller is amount but maybe just as important: constants (or literals) inserted into the code by the programmer.

Virtually every popular language nowadays has custom syntax to allow the programmer to describe some very common (and primitive) data types, such as numbers, booleans or strings. Many modern languages even have syntax for more advanced data structures, such as arrays and dictionaries (or hash tables).

When we cross into the territory of DSLs \textit{(Domain Specific Languages)}, there is a multitude of custom syntax for the most varied data types. In this paper we will discuss a proposed solution to the problem of describind musical notes, and more important, musical arrangements and melodies as data values that can be integrated into a programming language.

Such problem can be divided into two parts: the syntax used for describing the notes and the operators the compose them; and the semantics of the generated events, how they are stored in memory, and how their temporal properties are handled without forcing the programmer/user to manually type them. The former is a relatively easy problem, and to minimize the learning curve for new users, we adopted a simplified version of the very popular note declaration syntax from the ABC Notation project\cite{AbcNotation}, with some minor changes. The latter is the one we'll be discussing in greater detail.

The most straightforward way for generating such timed events is to let the user control the timing manually: setting both properties (start and duration) explicitly for each event, and maybe doing so with regular function calls, thus avoiding the need for extra syntax even. This is the approach used by some of the existing languages in this space, such as \textit{SonicPi}. But this approach is also cumbersome and prone to mistakes though, as one could guess from the following usecase: if the user wants to change one of the earlier events' timing, he would have to manually update the timings of all of the following events.

\begin{lstlisting}[caption={Example of a hypothetical imperative API for creating events},label=list:8-6,captionpos=t,abovecaptionskip=-\medskipamount]
play( 0, 100, 'A' );
play( 100, 50, 'B' );
play( 150, 200, 'C' );
\end{lstlisting}

Instead, we'll take a look at an alternative approach based on custom operators and syntax, each with their own semantic and syntax that enable an expressive way to define those events. We can look at this the same way languages implement other primitive values: most often we can declare static strings, not by manually setting each character in memory, but by writing the string wrapped in special characters. For musical events, we will be doing the same, exploring a way to define them in code, as \textit{musical literals}.

\begin{lstlisting}[caption={Our proposed declarative syntax that calculates timings implicitly},label=list:8-6,captionpos=t,abovecaptionskip=-\medskipamount]
play( A B/2 C2 );
\end{lstlisting}

But more that just being able to define those events, we will also be interested in exploring how well they integrate with existing and common programming language constructs, like variables, functions, loops and other control structures.

\section{The Problem and its Requirements}
There are two important requirements we need to consider when evaluating possible solutions to this problem: the ability to produce music interactively, and to produce music lazily.

The first requirement, \textbf{interactivity}, relates to our goal of not only being able to generate music offline, but also in a live environment: give the user the ability to program several snippets of musical events, and then control them through a virtual keyboard or through other interactive means.

The second requirement, \textbf{laziness}, refers to a concept that is familiar in functional programming languages: values are generated when we need them, not earlier. In our case, this implies that a musical sequence could be potentially infinite (like an infinite repetition of some arrangement). If playing this music live, the musician could determine when to stop this arrangement sooner or later.

Given these two requirements, we can conclude we cannot generate all music events at the start and then play them in order. 

\begin{lemma}[Total Order]
\label{lemma:total-order} All operators must return a sequence of events in respecting our time unit's total order.
\end{lemma}

\paragraph*{Data Model}
The basic premise is that expressions can generate a special data type: \textbf{Music}. Music is simply a sequence of ordered musical events.

A musical \textbf{Event} can be one of many things, such as a \textit{note}, a \textit{chord}, or even more implementation-specific events like MIDI messages. While all events must have a start time, some events can be instantaneous (events with a duration of zero).

The time unit used does not need to be a common time measure, like seconds or milliseconds, and can be really anything so long as it has a \textbf{total order}.

\paragraph*{Operators}
Operators are special operations defined at the syntactic level that allow \textit{music} to be composed in different ways, such as concatenated, parallelized or repeated.

\begin{description}
    \item[Concatenation] \verb|Music1 Music2 ... MusicN|
    \item[Parallel] \verb'Music1 | Music2 | ... | MusicN'
    \item[Repetition] \verb'Music * Integer'
\end{description}

It is also useful to estabilish that while most operators work on sequences of musical events, they can also accept a singular event as their argument: one event can be trivially converted into a sequence of one element. Such ocorrence is so common and trivial that the conversion should therefore be implicit.

\section{Implementation}
The reference implementation for this system is written in Python, although the approach here should be language agnostic.

One of the features that Python boasts (but are certainl not exclusive to it) that have eased our implementation are generators\cite{PEP255}. They integrate very nicely into both our concept of emitting musical events, as well as our concept of laziness where events that are not used are not generated either.

\paragraph*{Context State}
To keep track of the \textit{cursor} (the current timestamp where the next event should start) each operator in our language is implemented as a function call that receives an implicit \texttt{Context} object. While here we'll mostly focus just on the methods related to time management provided by the context, it can be used to store other types of information, like the default length of a musical note, to avoid forcing the user to type it out all the time.

Let's describe what kinds of functionality our context should provide.

\begin{description}
    \item[cursor(ctx)] Return the current cursor position
    \item[seek(ctx, time)] Advance the cursor to the given position
    \item[fork(ctx)] Clone the parent context and return the new one. Allows multiple concurrent contexts to be used
    \item[join(parent, child)] If the child's cursor is ahead, make the parent context catch up
\end{description}

\subsection{Operators}

\paragraph*{Note Events}
The basic building block of our system is going to be a \textbf{Note Event}. We can then add even more events, like chords and rests, while the type of the event might change, the semantics are equivalent.

The function that emits the event receives the current context as an argument, and is then responsible for creating the event, with its timestamp matching the context's cursor. After the event is created, it also must move the context's cursor forward through the \texttt{seek} method.
\begin{lstlisting}[caption={Creating a Note Event},label=list:8-6,captionpos=t,abovecaptionskip=-\medskipamount]
function note (ctx) {
    event = create_note(cursor(ctx));
    
    seek(ctx, event.duration);
    
    yield event;
}
\end{lstlisting}

\paragraph*{Concatenation}
We've seen how single events' creation is handled. Now it is important for us to see how we can combine those events together. And probably the most straightforward operator of all, concatenation, it simply consumes each event. Each event, as we've seen before, is responsible for seeking the context depending on the event's duration.
\begin{lstlisting}[caption={Algorithm to concatenate musical events},label=list:8-6,captionpos=t,abovecaptionskip=-\medskipamount]
function concatenate (ctx, ...operands) {
    for (music in operands) {
        for (event in music(ctx)) {
            yield event;
        }
    }
}
\end{lstlisting}
\paragraph*{Repetition}
The repetition operand is in a way very similar to the concatenation operator. It makes sense, since repeating any kind of music pattern $N$ times could be thought as a particular case of as concatenation where there are $N$ operands, all representing the same musical pattern.

\begin{lstlisting}[caption={Algorithm for repetition},label=list:8-6,captionpos=t,abovecaptionskip=-\medskipamount]
function repeat (ctx, music, count) {
    for (i = 0; i < count; i++) {
        for (event in music(ctx)) {
            yield event;
        }
    }
}
\end{lstlisting}


\paragraph*{Parallel}
The parallel operator enables playing multiple sequences of musical events simultaneously. However our events are emitted as a single sequence of ordered events, thus requiring merging the multiple sequences into a single one, while maintaining the properties of laziness and order. The operator assumes that each of its operands already maintains those properties on their own, and so is only in charge of making sure the merged sequence does so as well. With this in mind, it relies on a custom \textit{merge sorted} algorithm for iterables (not related to the most common merge sort algorithm by John von Neumann).

\begin{lstlisting}[caption={Algorithm to merge parallel musical events},label=list:8-6,captionpos=t,abovecaptionskip=-\medskipamount]
function parallel (ctx, ...operands) {
    for (child_ctx, event in merge_sorted(ctx, ...operands)) {
        join(ctx, child_ctx);
        
        yield event;
    }
}
\end{lstlisting}

The merge sorted function receives $N$ operands and creates a buffer with the size $N$. For each operand it \textit{forks} the context, so that they can execute concurrently and each will mutate their own context only. It then requests one single event for each operand.

After the buffer is prefilled (meaning it has at least one event for all non-empty operands), the algorithm finds the earliest event stored in the it. Let's assume it is stored in the $K$ index of the buffer, with $K < N$. The method emits the value stored in \texttt{buffer[K]} and then fills requests the next event from the $K$ operand (storing \texttt{null} if the operand has no more events to emit). It then repeats this step until all operands have been drained.

\subsection{Integration in a Programming Environment}
Apart from generating musical events from static instructions, our goal is to have those events integrate into a programming language in the same way integers, floats, strings and booleans do: as data that can be stored, passed around and manipulated. This, of course, while still retaining all the properties we've laid out for our sequences of events: being lazy and always being ordered.

\paragraph*{Variables}
Up until now we've analyzed situations where the timing of the events is known at the moment of their creation (through inspection of the context passed onto them). However, there can be situations where the events have to be created before their time location is known.

We decided to take an approach to integrate this type of functionality that does not require different semantics for creating events \textit{"live"} and for storing events in all operators. Instead, the only changes occur in variable declarations and when inserting variables in the middle of other expressions.

When declaring a variable, its expression is evaluated with custom context, forked from the main context, with the cursor reset back to zero. An important thing to note is that even though we are storing the music in a variable, we still take care to respect the principle of laziness present throughout our language: when the variable is declared, no actual events are created. Later, whenever the variable is used, and an event is requested of the variable, it relays that request to the expression responsible for providing the event.

However, one important caveat is that the variable cannot simply emit the events as they are created, because as we mentioned before, their context is a different one from the main context (with its cursor set to zero at the start of the expression).

Let's assume that a variable $C$ holds the value of the \texttt{cursor(ctx)}, it \texttt{ctx} is the context where the variable is being evaluated (not declared). Then, each event would be cloned and its starting time would have the constant $C$ added to it.

This raises an important detail that we must not forget: many of the musical events are created and then emitted directly. However, as we've seen, some can be stored in variables. And variables can be used in many ways, and the same variable can be used multiple times. This means that if one were to mutate an event (by change its start time or its duration, for instance), we would be changing the the contents of the variable. This could be an acceptable approach. But we have opted instead for a more functional approach, treating those musical events as more akin to primitive values (as we would treat numbers or event strings in modern languages). This means that when we change the start time of an event, we are actually cloning it.

This works well enough because those events are very lightweight objects, and the benefits of not having their values mysteriously changed outweight the small cost of a possible unnecessary allocation, in case that event was actually only used once.

\paragraph*{Functions}
When designing functions into our language, we decided to keep the semantics simple. Emitting events inside a function is similar to its return value being an iterator that gives out the emitted events on demand. This means that a value cannot both emit musical events, while also returning other values manually through a return statement.

There is no syntactic marker to distinguish regular functions from musical-emitting ones. Instead, the language runtime starts executing each function as a regular one, and automatically switches its execution mode into a generator-like implementation once the first event is emitted. Any return statements that are evaluated after this point must have no value (thus preserving the feature to early-stop a function). If they do try to return a custom value, a runtime exception is triggered.

In terms of managing the implicit context, functions are treated in very similar ways to the regular operators. They receive the context that was active at their call site and are evaluated with it. This means that any events emitted will have begin at whatever time the context's cursor marks.

Their arguments, however, are treated the same way as variable declarations (which is what they really are), meaning that each argument gets a custom fork of the call site's context with its \texttt{cursor} set to zero.

\section{Results Discussion}
\label{sec:conclusions}

To solve our problem of keeping track of the timing implicitly for each event created, we decided to pass around a context variable. There were other possible solutions, like keeping this data in some sort of global variable. Our approach does give us some advantages, such as beeing able to have multiple contexts in play at the same time. It does have drawbacks, too. Every function defined in our language must receive this context to be able to create events at the appropriate time. However, functions defined in Python do not expect this parameter. Therefore, special conversions must be made when exchanging values between both languages.

Also, our solution to have variables just offset the timings of each event they contain every time those variables are used simplifies the process of integrating variables into our existing semantics of music generation. This solution, however, does not answer other questions unrelated to the timing, such as: should events stored in variables use the musical instrument set when they were declared, or when the variable was used?

However, this early work already provides a solid foundation for a musical \textit{DSL} that while dynamic (with variables, functions and control structures) integrates very well with established musical standards such as the MIDI protocol and others.


\bibliography{references}

\end{document}
