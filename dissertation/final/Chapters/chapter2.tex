%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Estado da Arte}
Atualmente a produção de música é realizada utilizando programas com interfaces gráficas, geralmente denominados como \acrfull{daw}. A minha abordagem irá consistir em estudar formas de criar e tocar músicas ao vivo (e não só) através de uma \acrfull{dsl}, usando técnicas inspiradas nas linguagens de programação e no desenvolvimento de \textit{software}.

\section{Trabalho Relacionado}
Existem diversos tipos de linguagens usadas atualmente para produzir ou simplesmente descrever música. Algumas fazem uso do conceito de notas músicais, com recurso a algum sintetizador externo, para gerar os sons, enquanto outras funcionam com base na manipulação direta de ondes de som digitais para criar música. Algumas suportam apenas a descrição estática da música, enquanto outras permitem formas dinâmicas tais como funções, variáveis, estruturas de controlo e repetição, ou até mesmo algorítmos aleatórios que permitem gerar músicas diferentes a cada execução.

Iremos de seguida analisar algumas das soluções disponíveis atualmente, bem como comparar as funcionalidades que cada uma oferece ou não oferece em relação aos objetivos deste projeto.
\subsection{Alda}
O projeto \textbf{alda}~\cite{alda} é uma linguagem de música textual desenvolvida em \textit{JAVA} focada na simplicidade: o seu maior ponto de atração é apelar tanto a programadores com pouca experiência musical, bem como a músicos com pouca experiência com programação.
Apesar de ser anunciada como direcionada tanto a músicos como a programadores, a linguagem não suporta nenhum tipo de construções dinâmicas, como ciclos ou funções. Este tipo de funcionalidades, se necessário, requer o uso de uma linguagem de programação por cima, que poderia por exemplo, gerar o código \textit{alda} em \textit{runtime} através da manipulação de \textit{strings} antes de o executar. Isto significa que não é possível implementar composições interativas.

\subsubsection{Exemplos}
O exemplo seguinte demonstra um simples programa escrito em \textit{alda}, demonstrando: a seleção de um instrumento (\texttt{piano:}), a definição da oitava base (\texttt{o3}), um acorde com quatro notas (\texttt{c1/e/g/>c4}) em que a última se encontra uma oitava acima das outras.
\begin{lstlisting}[caption={Exemplo da linguagem alda}]
piano: o3 c1/e/g/>c4 < b a g | < g+1/b/>e
\end{lstlisting}
 É também possível verificar o uso de acidentais (identificados pelos símbolos \texttt{+} ou \texttt{-} a seguir a uma nota) bem como a diferenciação da duração de algumas notas (identificadas pelos números em frente às notas).
\subsection{ABC Notation}
A notação \textbf{ABC}~\cite{AbcPlus}~\cite{abc-notation} é uma notação textual que permite descrever notação músical. É bastante completa, tendo formas de descrever notas, acordes, acidentais, uniões de notas, \textit{lyrics}, múltiplas vozes, entre outros.

Para além das exaustividade de sintaxe que permite descrever quase todo o tipo de música, a popularidade da linguagem também significa que existem já inúmeros conversores de ficheiros ABC para os mais diversos formatos, desde ficheiros MIDI, pautas músicais, ou mesmo ficheiros WAV (gerados através do fornecimento de um ficheiro SoundFont, por exemplo).

A complexidade da notação traz tanto vantagens como desvantages, no entanto: A sua ubiquidade significa que uma maior percentagem de utilizadores já se pode sentir à vontade com a sintaxe, o que não acontece com outras linguagens menos conhecidas. Mas por outro lado, conhecer ou implementar toda a especificação~\cite{abc-notation-standard} é um feito bastante difícil.

No entanto, tal como a linguagem \textit{ALDA}, as músicas definidas são estáticas, pelo que não serve como uma linguagem de programação de músicas dinâmicas. Ainda assim, apesar de implementar toda a notação ser algo pouco prático, implementar um \textit{subset} da notação, contendo as construções mais usadas seria uma vantagem enorme que me permitiria aproveitar a familiaridade de muitos utilizadores com as partes mais comuns da sintaxe.

    \subsubsection{Exemplos}
A sintaxe de um ficheiro \textit{ABC} é composta por duas partes: um cabeçalho onde são definidas as configurações da música atual, seguido pelo corpo da música. O cabeçalho é formado por uma várias linhas. Cada linha, em ABC chamada de campo, tem uma chave e um valor separados por dois pontos (\texttt{:}). A especificação da notação descreve bastantes campos possíves, mas os mais usados são: \textbf{X} (número de referência), \textbf{T} (título), \textbf{M} (compasso), \textbf{L} (unidade de duração de nota) e \textbf{K} (clave).

\begin{lstlisting}[caption={Exemplo da notação ABC}]
C, D, E, F,|G, A, B, C|D E F G|A B c d|e f g a|b c' d' e'|f' g' a' b'|]
\end{lstlisting}

No exemplo acima podemos ver uma escala completa das notas (sem acidentais). O chamado C médio é representado por um \textbf{c} minúsculo (a capitalização das letras muda o significado). Para subir uma oitava, podemos anotar as notas com um apóstrofo (\textbf{c'}). As oitavas subsequentes são anotadas por mais apóstrofos. De modo análogo, para baixar uma oitava, devemos usar primeiro a nota em maíuscula (\textbf{C}). As oitavas anteriores são identificadas por uma (ou mais) vírgula a seguir à nota com letra maíuscula (\textbf{C,}).


\begin{lstlisting}[caption={Exemplo da notação ABC}]
A/2 A/ A A2 __A _A =A ^A ^^A [CEGc] [C2G2] [CE][DF]
\end{lstlisting}
A duração das notas pode ser ajustada relativamente à unidade global definida no cabeçalho acrescentando um número (por exemplo \textbf{2}) ou fração \textbf{1/4} à nota. Os acidentais bemol, bequadro e sustenido podem ser adicionados acrescentando um \textbf{\_}, \textbf{=} e \textbf{\^} antes da nota, respetivamente. Acordes (notas tocadas ao mesmo tempo) podem ser definidas entre parênteses retos (\textbf{[} e \textbf{]}).

A notação disponibiliza muitos mais exemplos de todas as funcionalidades aceites no seu website~\cite{abc-notation-examples}.

\subsection{Faust}
A linguagem \textbf{Faust}~\cite{orlarey:Faust}~\cite{faust} é uma linguagem de programação funcional com foco na sintetização de som e processamento de audio. Ao contrário das linguagens analisadas até agora, não trabalha com abstrações de notas e elementos músicais. Em vez disso, a linguagem trabalha diretamente com ondas sonoras (representadas como \textit{streams} de números) e através de expressões matemáticas, que de uma forma funcional permite assim manipular o som produzido.

Um dos pontos fortes da linguagem é o facto da sua arquitetura ser construída de raiz para compilar o mesmo código fonte em várias linguagens. De facto, o projeto conta com várias dezenas de \textit{targets}, desde os mais óbvios (C, C++, Java, JavaScript) até alguns mais especializados (WebAssembly, LLVM Bitcode, instrumentos VST/VSTi). Também permite gerar aplicações \textit{standalone} para as bibliotecas de audio mais comuns~\cite{faust-targets}.
	
A linguagem vem embutida com uma biblioteca extremamente completa~\cite{faust-libraries} que implementa, entre muitas outras, funções de matemática comuns, filtros audio, funcionalidades extremamente básicas de interfaces gráficas que permitem controlar em tempo real os valores do programa (como botões e \textit{sliders} entre outros).
	
\subsubsection{Exemplos}
A documentação do projeto conta com uma quantidade abundante de exemplos~\cite{faust-examples} e com um tutorial para iniciantes~\cite{faust-quickstart}, do qual irei colocar aqui alguns pequenos pedaços de código que demonstram as capacidades fundamentais da linguagem.

\begin{lstlisting}[caption={Geração de ruído aleatório com volume a metade},captionpos=b]
import("stdfaust.lib");
process = no.noise*0.5;
\end{lstlisting}

No primeiro exemplo, podemos ver a estrutura mais básica de um programa escrito em \textit{Faust}. Na primeira linha é importada a biblioteca \textit{standard} da linguagem. Na segunda linha podemos ver a \textit{keyword} \textbf{\texttt{process}}, que representa o \textit{input} e \textit{output} audio do nosso programa. Finalmente, em frente a essa \textit{keyword} podemos ver a expressão \texttt{no.noise*0.5}. Isto demonstra a utilização de construções da biblioteca \textit{standard}, como o gerador de ruído aleatório, bem como a utilização de operadores matemáticos usuais (neste caso a multiplicação) para manipular o audio, e diminuir o volume para metade.

\begin{lstlisting}[caption={Geração de ruído aleatório com um filtro \textit{low-pass}}]
import("stdfaust.lib");
ctFreq = 500;
q = 5;
gain = 1;
process = no.noise : fi.resonlp(ctFreq,q,gain);
\end{lstlisting}
Neste exemplo, estamos a usar o operador \texttt{:} para canalizar o \texttt{output} do gerador de ruído para um filtro \textit{low-pass}, que filtra todas as frequências acima de um valor de corte (a variável \texttt{ctFreq}). Aumentar esta variável resulta num som mais agudo, enquanto que ao diminui-la obtemos um som mais grave (pois o valor de corte é mais baixo, apenas os sons abaixo desse valor são passados).

\begin{lstlisting}[caption={Geração de ruído aleatório com um filtro \textit{low-pass} controlada por uma interface}]
import("stdfaust.lib");
ctFreq = hslider("[0]cutoffFrequency",500,50,10000,0.01);
q = hslider("[1]q",5,1,30,0.1);
gain = hslider("[2]gain",1,0,1,0.01);
t = button("[3]gate");
process = no.noise : fi.resonlp(ctFreq,q,gain)*t;
\end{lstlisting}
Por fim podemos ver um exemplo igual ao anterior, mas em vez de ter os valores das variáveis estáticos (guardados nas variáveis \texttt{ctFreq}, \texttt{q} e \texttt{gain}), estes são controlados em tempo real pela interface definida pelas chamadas à função \texttt{hslider}. Foi também adicionada uma variável \texttt{t} com um botão \textit{"gate"}. Este produz o valor 0 (zero) quando está solto, e o valor 1 (um) quando está pressionado, valor que quando multiplicado pelo resto da expressão serve efetivamente como um \textit{on-off switch} para todo o sistema.

\subsection{SuperCollider}
O projeto \textbf{SuperCollider}~\cite{doi:SuperCollider} é uma plataforma para geração e sintetização de som e música. É composta não só pela linguagem interpretada \textbf{sclang} focada na componente de aúdio, mas não limitada a isso. Também tem um servidor de aúdio \textit{realtime} \textbf{scsynth}, que pode ser controlado pela linguagem \textit{sclang}, e que implementa diversas técnicas de geração de aúdio (permitindo ao utilizador programar também as suas próprias técnicas através em C++. Também integra um \acrshort{ide} \textbf{scide} que disponibiliza um ambiente de edição integrado para todo o ecossistema.

É uma linguagem de baixo nível mas com um grande ecossistema para integrar os mais diversos componentes, tais como controladores MIDI ou interfaces gráficas. Mas a nível músical, foca-se na sintetização e manipulação de som digital, sem ter noção de conceitos mais abstratos como notas ou acordes. Tais noções têm de ser manualmente implementadas pelo utilizador, e de uma forma bastante mais verbosa do que o desejável.

\subsubsection{Exemplos}
A linguagem do projeto \textit{sclang} é uma linguagem orientada a objetos mas com aspetos funcionais (como \textit{currying} ou listas em compreensão).

\begin{lstlisting}[caption={Declaração de dois canais de aúdio com base em dois osciladores},label={lst:sc-1}]
{ [SinOsc.ar(440, 0, 0.2), SinOsc.ar(442, 0, 0.2)] }.play;
\end{lstlisting}

No exemplo presente na listagem \ref{lst:sc-1}, é declarada uma função (demarcada pelo par de chavetas) que retorna uma lista com dois osciladores de ondas sinusoidais. O som dos osciladores é depois reproduzido através da chamada da função \texttt{play}. De notar que como os osciladores estão dentro de um \textit{array}, isso permite implementar múltiplos canais de aúdio, com um oscilador para cada canal.

O oscilador \texttt{SinOsc} é apenas um dos geradores de som disponibilizados pelo servidor \textit{scsynth}, também chamados \textbf{UGens}. Existem outros, e mais importante, é possível compor esses geradores para criar sons mais complexos.

Um exemplo ainda simples desse tipo de composição, presente na listagem \ref{lst:sc-2} seria usando o gerador \textit{Pan2} que redireciona o som de outro gerador qualquer para dois canais diferentes. A prevalência do som em cada canal pode ser costumizada por um segundo argumento, com um valor entre -1 e 1, onde -1 emitiria apenas som no canal da esquerda, 1 emitira apenas som no canal da direita, e qualquer valor pelo meio iria produzir uma gradação, dependendo desse valor, nos dois canais.

\begin{lstlisting}[caption={Dividir um gerador por dois canais de forma desigual},label={lst:sc-2}]
{ Pan2.ar(PinkNoise.ar(0.2), -0.3) }.play;
\end{lstlisting}

Uma versão mais interessante deste exemplo é possível quando sabemos que os geradores podem ser utilizados não só para gerar som, mas também para servirem de parâmetros a outros geradores. Por exemplo, na listagem \ref{lst:sc-3}, em vez de passarmos uma literal $-0.3$ como segundo argumento, podemos passar um oscilador. Desta forma, o som gerado pelo \texttt{PinkNoise} irá variar em proporção ao longo do tempo pelos dois canais gerados, em vez de tocar de forma fixa no mesmo.

\begin{lstlisting}[caption={Dividir um gerador por dois canais de forma desigual},label={lst:sc-3}]
{ Pan2.ar(PinkNoise.ar(0.2), SinOsc.kr(0.5)) }.play;
\end{lstlisting}

A linguagem é bastante mais complexa, podendo declarar variáveis, objetos, executar funções, estruturas de controlo como condicionais e ciclos, e muito mais. Mas relativamente à parte musical, contém ferramentas bastante poderosas mas apenas de baixo nível, com manipulação da música como apenas ondas sonoras.

\subsection{ChucK}
A linguagem \textbf{ChucK}~\cite{doi:Chuck} é outra linguagem de sintetização de áudio digital, similar aos projetos \textit{SuperCollider} e \textit{Faust}. O seu maior fator de diferenciação advém da sua abordagem única e interessante de sincronização de processos concorrentes baseado em unidades de tempo (que os autores do projeto denominaram \textbf{strongly-timed}).

Este conceito significa que o utilizador pode definir tempos virtuais associados a quando certas instruções devem ocorrer. Tal garantia torna-se útil quando combinada com o conceito de \textit{shreds} (processos virtuais que podem estar a correr concurrentemente (e dando a aparência para o utilizador que estão na verdade a correr em paralelo). A máquina virtual por trás da linguagem, responsável por traduzir as instruções em aúdio, assegura-se que os \textit{shreds} são \textbf{sample-synchronous}, ou seja, cada \textit{sample} (ou amostra) geradas pelos \textit{shreds} e com o mesmo \textit{timestamp} virtual irão ser sempre reproduzidas ao mesmo tempo (mesmo que o processador tenha demorado mais tempo a correr um dos \textit{shreds} do que os outros). 

Mais uma vez esta linguagem lida com o conceito de geração de áudio a um baixo nível, e não é portanto muito adequada para a descrição de notação musical. Apesar disso, este projeto apresenta também um fator diferenciador que permite integrar a sua componente de agendamento temporal para permitir interação (através de eventos \textit{MIDI} ou do teclado do computador). Mais uma vez, mesmo estas funcionalidades são de baixo nível (como iremos abordar mais em detalhe nos exemplos) mas é agradável saber que já vêm pelo menos incluídas com a linguagem.

\subsubsection{Exemplos}
A operação central da linguagem \textit{ChucK} passa pelo operador \texttt{=>} (também chamado de operador \textit{chucking}). A sua semântica pode ser pensada um pouco como a atribuição a variáveis, ou \textit{piping} de dados. Por exemplo, na listagem \ref{lst:chuck-1} podemos ver a declaração de um oscilador sinusoidal \textbf{\texttt{SinOsc s}} que é \textit{chucked} para a variável \textbf{\texttt{dac}}. Esta é uma variável especial da linguagem e que representa o dispositivo de reprodução de aúdio (as colunas ou auscultadores).

Na linha seguinte podemos ver também que é atribuída à variável \textbf{\texttt{now}} o valor de dois segundos: esta é outra variável especial da linguagem, neste caso responsável por controlar o agendamento da execução de código. Ao executar essa instrução, estamos efetivamente a parar a execução da \textit{shred} atual durante dois segundos (ficando o som do oscilador definido atrás a reproduzir som durante os dois segundos para o dispositivo de aúdio).
\begin{lstlisting}[caption={Reproduzir um oscilador durante dois segundos},label={lst:chuck-1}]
SinOsc s => dac;

2::second => now;
\end{lstlisting}

Ao fim dos dois segundos, como o programa não tem mais nenhuma instrução, a reprodução de som termina. Se quiséssemos reproduzir som infinitamente, poderíamos mover a instrução de avançar no tempo para dentro de um ciclo, como vemos na listagem \ref{lst:chuck-2} (criando o que é chamado de \textit{time-loop}, ou neste caso em particular, um \textit{time-loop infinito}).

\begin{lstlisting}[caption={Reproduzir um oscilador infinitamente},label={lst:chuck-2}]
SinOsc s => dac;

while( true ) {
    2::second => now;
}
\end{lstlisting}

Neste caso o nosso \textit{time-loop} não faz nada senão esperar continuamente. Mas a sua utilidade é revelada quando vemos que podemos mudar as propriedades do som gerado ao longo do tempo. Por exemplo, podemos verificar que para além de conectar o oscilador ao dispositivo aúdio, também lhe demos um nome \textbf{s}. Assim conseguimos usar esse nome para alterar, dentro do ciclo, a sua frequência, por exemplo, de dois em dois segundos, como vemos na listagem \ref{lst:chuck-3}.
\begin{lstlisting}[caption={Reproduzir um oscilador, variando a frequência a cada 2 segundos},label={lst:chuck-3}]
SinOsc s => dac;

while( true ) {
    2::second => now;
    Std.rand2f(30.0, 1000.0) => s.freq;
}
\end{lstlisting}

Neste caso geramos um valor aleatório entre $30.0$ e $1000.0$, e associamos esse valor à frequência do gerador utilizado. Para já temos apenas agendado a execução do código a cada dois segundos. Mas podemos avançar o tempo por qualquer duração, para um tempo específico, ou até avançar o tempo com a precisão de \textit{sub-samples}.
\\
\begin{lstlisting}[caption={Exemplos de instruções de avanço no tempo},label={lst:chuck-4}]
1::second => now;
100::ms => now;
1::samp => now;
.024::samp => now;
\end{lstlisting}

Mas algo bastante interessante sobre a variável \textbf{now} é que pode esperar não só por durações de tempo pré-definidas, mas também por eventos interativos que não sabemos quando irão acontecer. É desta forma que é implementada a interatividade com o teclado do computador, por exemplo, ou até mesmo com portas MIDI.

\begin{lstlisting}[caption={Exemplos de instruções de avanço no tempo},label={lst:chuck-5}]
Hid hi; HidMsg msg;

0 => int device;
if( me.args() ) me.arg(0) => Std.atoi => device;

if( !hi.openKeyboard( device ) ) me.exit();
<<< "keyboard '" + hi.name() + "' ready", "" >>>;

while( true ) {
    hi => now;

    while( hi.recv( msg ) ) {
        if( msg.isButtonDown() ) { <<< "down:", msg.key >>>; } 
        else { <<< "up:", msg.key >>>; }
    }
}
\end{lstlisting}

O exemplo é bastante grande e verboso, e vai de encontro à nossa opinião sobre apesar de permitir interagir com teclados, é de bastante baixo nível. Se quisermos ter ações associadas a diferentes teclas, temos de construir estruturas de controlo para decidir qual das teclas foi premida e o que fazer. Então se quisermos reagir não só a uma tecla, mas a uma tecla com modificadores (por exemplo \texttt{Ctrl Shift T}) temos de guardar manualmente o estado das teclas premidas e saber quando podemos despoletar a ação.

Para além disso, se quisessemos associar algumas ações a teclas do computador, e outras a teclas de um piano MIDI, teríamos de manualmente abrir os dois dispositivos, e depois arranjar alguma forma de esperar concurrentemente pelos eventos de ambos (possivelmente usando uma \textit{shred} diferente para cada dispositivo).
Em conclusão, não é uma interface agradável para quem queira fazer umas rápidas experimentações músicais, principalmente alguém que não seja músico e não um programador mais experiente.

\subsection{Sonic Pi}
Possivelmente a linguagem que mais se aproxima do objetivo pretendido com este projeto, \textbf{Sonic Pi}~\cite{doi:SonicPi}~\cite{sonic-pi} descreve-se como uma ferramenta de código para a criação e performance de música.

A linguagem permite tocar notas (e também construções mais complexas a partir das mesmas, tais como acordes, arpégios e escalas, por exemplo). Para além disso permite tocar \textit{samples}, que são ficheiros \acrfull{wav}. A linguagem traz já consigo aproximadamente 164 \textit{samples} que podem ser livremente usadas, mas é também possível ao utilizador usar as suas.

As músicas são compostas por \textbf{live loops}, que são grupos de sons. É possível ter vários \textit{live loops} a tocar simultâneamente. Dentro de cada \textit{live loop} o utilizador pode usar a função \texttt{play} para tocar notas, \texttt{sample} para reproduzir ficheiros \acrshort{wav}, ou \textit{sleep} para avançar o tempo. Para além disso a linguagem suporta, através da função \texttt{with\_fx} a reprodução de sons com efeitos (como \textit{reverb}, \textit{pan}, \textit{echo} entre muitos outros~\cite{sonic-pi-fx}).

Para além das capacidades músicais, a linguagem disponibiliza numa sintaxe similar a \textit{Ruby}, com construções de programação como ciclos, variáveis, estruturas de controlo, e até métodos para adicionar aleatoriedade à música tocada, permitindo escolher, por exemplo, qual a nota a tocar a seguir de uma lista de possibilidades.

Apesar de tudas estas funcionalidades disponibilizadas, existem áreas onde o \textit{Sonic Pi} fica aquém dos objetivos pretendidos para este projeto. Por exemplo, apesar de permitir tanto receber como enviar eventos \acrshort{midi}, as suas capacidades de \acrfull{io} são bastante primitivas. Também não é fácil utilizar o teclado teclado do computador para tocar sons ou manipular o estado do programa. É possível fazê-lo, mas é bastante mais complicado do que seria de esperar (para além de exigir utilizar alguma linguagem de programação à parte).

\subsubsection{Exemplos}
\begin{lstlisting}[caption={Reproduzir um \textit{sample} com valores aleatórios}] 
loop do
    sample :perc_bell, rate: (rrand 0.125, 1.5)
    sleep rrand(0, 2)
end
\end{lstlisting}
Neste exemplo, podemos ver como a linguagem \textit{Sonic Pi} permite criar um \textit{loop}, onde podemos tocar sons (neste caso, um sample pré-definido chamado \texttt{perc\_bell}).

É possível verificar também o uso da função \texttt{sleep} para gerir manualmente o avanço temporal da música (neste caso usando um valor escolhido aleatóriamente e entre 0 e 2 segundos).


\begin{lstlisting}[caption={Reproduzir um notas de uma escala aleatórias, com efeito \textit{reverb}}] 
with_fx :reverb, mix: 0.2 do
    loop do
        play scale(:Eb2, :major_pentatonic, num_octaves: 3).choose, release: 0.1, amp: rand
        sleep 0.1
    end
end
\end{lstlisting}
Neste exemplo podemos observar a possibilidade do uso de efeitos, em particular do efeito \texttt{reverb}, para manipular o som gerado pelo programa. Dentro do \textit{loop}, é tocada uma nota a cada 100 milisegundos. A função \texttt{scale} gera uma lista com as notas da escala pedida, e a função \texttt{choose} escolhe aleatóriamente uma dessas notas para tocar.
    
\section{Gramáticas}
	%% PEG vs CFG:
%%  - Em PEGs, o operador de escolha / é ordenado, i.e. é relevante a ordem por que as várias alternativas são colocadas.
%%    Numa CFG, o operador de escolha não é ordenado, o que pode resultar em gramáticas ambíguas, onde o mesmo input pode resultar em diferentes parse trees.
%% https://stackoverflow.com/questions/34976533/some-information-about-peg-needed
%% https://en.wikipedia.org/wiki/Parsing_expression_grammar
%% https://compilers.iecc.com/comparch/article/05-09-114


Para além dos aspetos técnicos da geração e reprodução de música já abordados neste relatório, existe também um componente fulcral relativo à análise e interpretação da linguagem que irá controlar a geração dos sons. Uma das primeiras decisões a ser tomada diz respeito à escolha do \textit{parser}, e possivelmente, do tipo de gramática que irá servir de base para a geração do mesmo.

Tradicionalmente, as gramáticas mais populares no campo de processamento de texto tendem a ser \acrfull{cfg}, que são usadas como \textit{input} nos geradores de \textit{parser} mais populares (Bison/YACC, ANTLR). Existem no entanto alternativas, algumas até mais recentes, como as \acrfull{peg}, que trazem consigo diferenças que podem ser consideradas por alguns como vantagens ou desvantagens.

\subsection{Diferenças: \acrshort{cfg} vs \acrshort{peg}}

A diferença com maiores repercussões práticas entre as duas classes de gramáticas deve-se á semãntica atribuída ao operador de escolha, e a consequente \textbf{ambíguidade} (ou falta dela) na gramática. Nas gramáticas \acrshort{peg}, o operador é ordenado, o que significa que a ordem por que as alternativas aparecem é relevante durante o \textit{parsing} do \textit{input}. Isto contrasta com a semântica nas \acrshort{cfg}, onde a ordem das alternativas é irrelevante. Isto pode no entanto levar a ambíguidades, onde o mesmo \textit{input}, descrito pela mesma gramática, pode resultar em duas árvores de \textit{parsing} diferentes, se satisfizesse mais do que um dos ramos do operador de escolha. Isto é, as \acrshort{cfg} podem por essa razão ser ambíguas.

Tomemos como exemplo o famoso problema do \textit{dangling else}~\cite{dangling-else} descrito nas duas classe de gramáticas:
\begin{lstlisting}
 if (a) if (b) f1(); else f2();
\end{lstlisting}

\begin{lstlisting}[caption=Gramática,captionpos=t]
 statement = ...
    | conditional_statement
 
 conditional_statement = ...
    | IF ( expression ) statement ELSE statement
    | IF ( expression ) statement
\end{lstlisting}

No caso de uma \acrshort{cfg}, sabendo que o operador de escolha \texttt{|} é comutativo, o seguinte \textit{input} será ambíguo, podendo resultar num \textit{if-else} dentro do \textit{if} ou num \textit{if} dentro de um \textit{if-else}.

Mas no caso de uma \acrshort{peg}, o resultado é claro: um \textit{if-else} dentro de um \textit{if}. Quando a primeira regra do condicional chega ao \texttt{statement}, este vai por sua vez chamar o não terminal \texttt{conditional\_statement}, que por sua vez irá consumir o \textit{input} até ao fim. Deste modo, quando a execução voltar ao primeiro \texttt{conditional\_statement}, esta irá falhar por não conseguir ler o \textit{else} (uma vez que já consumimos todo o texto de entrada). Irá depois irá usar a segunda alternativa, dando então o resultado previso.

Com este exemplo de \textit{backtracking} podemos também verificar um problema aparente nas gramáticas \acrshort{peg}. Falhando a primeira alternativa na produção \texttt{conditional\_statement}, a segunda irá ser testada. Mas é evidente, olhando para a gramática que a segunda alternativa é exatamente igual à parte inicial da primeira alternativa (que neste caso também corresponde á parte que teve sucesso). Em vez de voltar a testar as regras de uma forma \textit{naive}, as \acrlong{peg} guardam antes em \textit{cache} os resultados de testes anteriores, permitindo assim uma pesquisa em tempo linear relativamente ao tamanho do \textit{input}, à custa de uma maior utilização de memória.

%% TODO Talk about composition and why PEG grammars can do it better than CFG

\subsection{Resumo}
Em resumo, as três principais diferenças entre as tradicionais \acrfull{cfg} e as mais recentes \acrfull{peg} são:

\textbf{Ambiguidade}. O operador de escolha ser comutativo nas \acrshort{cfg} resulta em gramáticas que podem ser ambíguas para o mesmo \textit{input}. As \acrshort{peg} são determínisticas, mas exigem mais cuidado na ordem das produções, uma vez que tal afeta a semântica da gramática.

\textbf{Memoization} Para evitar \textit{backtracking} exponêncial, as \acrshort{peg} utilizam \textit{memoization} que lhes permite guardar em \textit{cache} resultados parciais durante o processo de \textit{parsing}. Isto reduz o tempo dispendido, pois evita fazer o \textit{parse} do mesmo texto pela mesma regra duas vezes. Mas também aumenta o consumo de memória, pois os resultados parciais têm de ser guardados até a análise terminar por completo.

\textbf{Composição} As \acrlong{peg} também têm a vantagem de oferecerem uma maior facilidade de composição. Em qualquer parte da gramática é possível trocar um terminal por um não terminal. Isto é, é extremamente fácil construir gramáticas mais modulares e compô-las entre si.

\section{SoundFonts}
O formato \textit{SoundFont} foi originalmente desenvolvido nos anos 90 pela empresa E-mu Systems para ser usado inicialmente pelas placas de som \textit{Sound Blaster}. Ao longo dos anos o formato sofreu diversas alterações, encontrando-se atualemente na versão 2.04, lançada em 2005~\cite{soundfont}. Atualmente existem diversos sintetizadores de software \textit{cross platform} e \textit{open source} capazes de converterem eventos \textit{MIDI} em som usando ficheiros \textit{SoundFont}, dispensando a necessidade de uma placa de som compatível com o formato. Alguns destes projetos são TiMidity\verb|++|~\cite{timidity}, WildMIDI~\cite{wild-midi} e FluidSynth~\cite{fluidsynth}.

Para além do formato original, existem também alternativas mais recentes que disponibilizam mais funcionalidades na sua especificação, como os ficheiros \textbf{SFZ} ou \textbf{NKI}. Estas alternativas trazem consigo vantagens e desvantagens, mas independentemente dos seus méritos, até agora nenhuma atingiu a popularidade dos ficheiros \textit{SoundFont}, o que significa também menos bibliotecas e menos aplicações para trabalhar com elas.

Um ficheiro de SoundFont é constítuido por um ou mais bancos (\textit{banks}) (até um máximo de 128). Cada banco pode por sua vez ter até 128 \textit{presets} (por vezes também chamados instrumentos ou programas).

Usando a sintaxe de declaração de tipos do \textit{Python} (que irá ser usada mais vezes neste projeto), podemos declarar um \textit{SoundFont}, de uma forma bastante genérica e omitindo detalhes não essenciais, como sendo modelado pelos seguintes tipos:
\newpage
\begin{lstlisting}[caption=Sistema de Tipos de um ficheiro SoundFont,language=Python]
    # Cada preset e identificado por um par: (bank, preset number)
    SoundFont = Dict[ Tuple[ int, int ], Preset ]
    
    # Cada instrumento e identificado por um inteiro entre 0 e 127
    Preset = Dict[ int, Instrument ]
    
    # Finalmente, cada sample e identificada pelo indice de nota (que iremos abordar mais a frente em detalhe, no capitulo sobre sintetizadores)
    Instrument = Dict[ int, Sample ]
    
    # Aqui nao se encontram detalhados os tipos Wave nem SampleOptions.
    Sample = Tuple[ Wave, SampleOptions ]
\end{lstlisting}


\section{Sintetizadores}
A biblioteca FluidSynth é um \textit{software} sintetizador de aúdio em tempo real que transforma dados MIDI em sons, que podem ser gravados em disco ou encaminhados diretamente para um \textit{output} de aúdio. Os sons são gerados com recurso a SoundFonts~\cite{soundfont} (ficheiros com a extensão \texttt{.sf2}) que mapeiam cada nota para a gravação de um instrumento a tocar essa nota.

Os \textit{bindings} da biblioteca para Python foram baseados no código \textit{open source} do projeto \textbf{pyfluidsynth}~\cite{pyfluidsynth}, jutamente com algumas definições CPython extra para permitir usar funções que não tivessem \textit{bindings} já criados.

\subsection{Inicialização}
Para utilizar a biblioteca FluidSynth, existem três objetos principais que devem ser criados: Settings (\texttt{fluid\_settings\_t*}), Synth (\texttt{fluid\_synth\_t*}) e AudioDriver (\texttt{fluid\_audio\_driver\_t*}).

O objeto \textbf{Settings}~\cite{fluidsynth_settings} é implementado com recurso a um dicionário. Para cada chave (por exemplo, \texttt{``audio.driver''}) é possível associar um valor do tipo inteiro (\texttt{int}), \textit{string} (\texttt{str}) ou \textit{double} (\texttt{num}). Alguns valores podem ser também booleanos (\texttt{bool}), no entanto eles são armazenados como inteiros com os valores aceites sendo apenas 0 e 1.

O objeto \textbf{Synth} é utilizado para controlar o sintetizador e produzir os sons. Para isso é possível enviar as mensagens MIDI tais como \texttt{NoteOn}, \texttt{NoteOff}, \texttt{ProgramChange}, entre outros.

O terceiro objeto \textbf{AudioDriver} encaminha automaticamente os sons para algum \textit{audio output}, seja ele colunas no computador ou um ficheiro em disco. Os seguintes \textit{outputs} são suportados pela biblioteca:

\begin{adjustwidth}{20pt}{20pt}
\begin{description}
    \item[Linux:] jack, alsa, oss, PulseAudio, portaudio, sdl2, file
    \item[Windows:] jack, PulseAudio, dsound, portaudio, sdl2, file
    \item[Max OS:] jack, PulseAudio, coreaudio, portaudio, sndman, sdl2, file
    \item[Android:] opensles, oboe, file
\end{description}
\end{adjustwidth}

\subsection{Utilização}
Com os objetos necessários inicializados, é necessário ainda especificar qual (ou quais) a(s) \textit{SoundFont(s)} a utilizar. Para isso podemos chamar o método \texttt{Synth.LoadSoundFont} que recebe dois argumentos: uma \textit{string} com o caminho em disco do ficheiro \textit{SoundFont} a carregar, seguido dum booleano que indica se os \textit{presets} devem ser atualizados para os da nova \textit{SoundFont} (isto é, atribuir os instrumentos da \textit{SoundFont} aos canais automaticamente).

A função \texttt{Synth.NoteOn} recebe três argumentos: um inteiro a representar o canal, outro inteiro entre 0 e 127 a representar a nota, e finalmente outro inteiro também entre 0 e 127 a representar a velocidade da nota.

O canal (\textbf{channel}) representa qual o instrumento que vai reproduzir a nota em questão. Cada canal está atríbuido a um programa da SoundFont, e é possível a qualquer momento mudar o programa atribuido a qualquer canal através do método \texttt{Synth.ProgramChange}. Caso se tenha carregado mais do que uma \textit{SoundFont}, é possível usar o método \texttt{Synth.ProgramSelect}, que permite especificar o \texttt{id} da \textit{SoundFont} e do banco do instrumento a atribuir.

A chave (\textbf{key}) representa a nota a tocar. Sendo este valor um inteiro entre 0 e 127, é necessário saber como mapear as tradicionais notas músicais neste valor. Para isso, basta colocarmos as \textit{pich classes} e os seus respetivos acidentais \textit{sharp} numa lista ordenada (\verb|C, C#, D, D#, E, F, F#, G, G#, A, A#, B|) e associar a eles os inteiros entre 0 e 11 (inclusive). Depois apenas temos de somar a esse número a multiplicação da oitava da nota (a começar em 0) por 12. Podemos deste modo calcular, por exemplo, que a \textit{key} do C central (C4) é igual a 48 ($0 + 4 * 12$). Assim, podemos generalizar que para uma oitava $O$ e para um tom de nota $N$, obtemos a chave aplicando a fórmula:
$$N + O * 12$$

A velocidade (\textbf{velocity}) é também um valor entre 0 e 127. Relacionando a velocidade com um piano físico, esta representa a força (ou velocidade) com que a tecla foi premida. Velocidades maiores geram sons mais altos, enquanto que velocidades mais baixas geram sons mais baixos, permitindo assim ao músico dar ou tirar enfase a uma nota relativamente às restantes. De notar que um valor igual a zero é o equivalente a invocar o método \texttt{Synth.NoteOff}.

A método \texttt{Synth.NoteOff}, por sua vez, recebe apenas dois argumentos (canal e chave), e deve ser chamada passsado algum tempo para terminar a nota. Podemos deste modo construir a analogia óbvia que o método \texttt{NoteOn} corresponde a uma tecla de piano ser premida, e \texttt{NoteOff} corresponde a essa tecla ser libertada.
